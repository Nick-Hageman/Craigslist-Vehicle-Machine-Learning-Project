{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pre-Analysis Data Considerations: Yes there were missing values. The way that I dealt with them depended on each faeture. I will go into further detail about how I addressed modifying my features in the markdown below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Project 2 Nick Hageman\n",
    "import csv\n",
    "%matplotlib inline\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib as mpl\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "from sklearn import tree\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn import (datasets, neighbors,\n",
    "                     naive_bayes,\n",
    "                     model_selection as skms,\n",
    "                     linear_model, dummy,\n",
    "                     metrics,\n",
    "                     pipeline,\n",
    "                     preprocessing as skpre)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Feature Engineering pt1: The below cell are the chosen features for the model. I tried to choose only the features that would be good in differentiating the vehicles in relation to it's price to improve it's predictions. I'll explain what I did to change the NAN data or strings in the next markdown where I modified all the data for the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#Choosing features\n",
    "features = [\n",
    "    \"year\",\n",
    "    \"cylinders\",\n",
    "    \"condition\",\n",
    "    \"odometer\",\n",
    "    \"title_status\",\n",
    "    \"price\"\n",
    "]\n",
    "\n",
    "features2 = [\n",
    "    \"year\",\n",
    "    \"cylinders\",\n",
    "    \"condition\",\n",
    "    \"odometer\",\n",
    "    \"title_status\",\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Feature Engineering pt2: To begin, I modified most of the features that included strings by replacing each value with a numerical value that I thought would give the features a sense of hierarchy in value. For example, for condition, I replaced the strings of the best condition with the highest numerical value. I did something similar with cylinders, giving the highest number of cylinders the highest numerical value. For title status, I did the same thing as I did with the condition. Now that all of these features had numerical values, I filled the NAN values with their respective means. For the rows in the odometer and year that contained NAN values, I found that I got the best RMSE when I just dropped those rows altogether rather than replacing it with the mean. Finally, I did one-hot encoding for the type of car, I chose this feature because I thought it would have a big impact and it turned out to improve my score significantly."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Another thing to note is that I had the \"data_train_drop\" data frame so when I dropped the odometer and year rows with NAN values, it would also drop the corresponding price so the data didn't get all discombobulated."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "data_train_df = pd.read_csv(\"cars_reg_train.csv\") \n",
    "#Make this temporary feature dataframe that INCLUDES the target so when I drop a row, it also drops the corresponding features to conserve the sizes of the final dataframes\n",
    "data_train_drop = data_train_df[features]\n",
    "\n",
    "# Modifying any rows with missing/string values\n",
    "data_train_drop[\"condition\"] = data_train_drop[\"condition\"].replace(\"new\", 5).replace(\"like new\", 4).replace(\"excellent\", 3).replace(\"good\", 2).replace(\"fair\", 1).replace(\"salvage\", 0).astype(float)\n",
    "data_train_drop[\"condition\"].fillna(data_train_drop[\"condition\"].mean(), inplace = True)\n",
    "data_train_drop[\"cylinders\"] = data_train_drop[\"cylinders\"].replace(\"12 cylinders\", 12).replace(\"10 cylinders\", 10).replace(\"8 cylinders\", 8).replace(\"6 cylinders\", 6).replace(\"5 cylinders\", 5).replace(\"4 cylinders\", 4).replace(\"3 cylinders\", 3).replace(\"other\", 0).astype(float)\n",
    "data_train_drop[\"cylinders\"].fillna(data_train_drop[\"cylinders\"].mean(), inplace = True)\n",
    "data_train_drop[\"title_status\"] = data_train_drop[\"title_status\"].replace(\"clean\", 3).replace(\"rebuilt\", 2).replace(\"salvage\", 1).replace(\"missing\", 0).replace(\"parts only\", 0).replace(\"lien\", 0).astype(float)\n",
    "data_train_drop[\"title_status\"].fillna(data_train_drop[\"title_status\"].mean(), inplace = True)\n",
    "\n",
    "#onehot function to create one-hot dataframes\n",
    "def onehot(x,str):\n",
    "    onehot = x[['Unnamed: 0',str]]\n",
    "    encoder = OneHotEncoder(handle_unknown='ignore')\n",
    "    encoder_df = pd.DataFrame(encoder.fit_transform(onehot[[str]]).toarray())\n",
    "    final_df = onehot.join(encoder_df)\n",
    "    final_df.drop('Unnamed: 0', axis=1, inplace=True)\n",
    "    final_df.drop(str, axis=1, inplace=True)\n",
    "    return(final_df)\n",
    "\n",
    "# working the onehot function for each column\n",
    "kind = onehot(data_train_df, 'type')\n",
    "#Assigning values so I don't have to write it all out again\n",
    "f = [\"1\", \"2\", \"3\", \"4\", \"5\", \"6\", \"7\", \"8\", \"9\", \"10\", \"11\", \"12\", \"13\"]\n",
    "kind.columns = f\n",
    "\n",
    "#Remove all rows with NaN values\n",
    "data_train_drop.dropna(inplace=True)\n",
    "data_train_df = data_train_drop.copy()\n",
    "\n",
    "#converts to float16 types\n",
    "for feature in features:\n",
    "    if feature != 'price':\n",
    "        data_train_df[feature] = data_train_df[feature].convert_dtypes(np.float16)\n",
    "        data_train_df[feature] = data_train_df[feature].convert_dtypes(np.float16)\n",
    "\n",
    "data_train_ft = pd.DataFrame(data_train_df[features2])\n",
    "#Adding each column back to the original df\n",
    "for col in f:\n",
    "    data_train_ft[col] = kind[col]\n",
    "data_train_tgt = data_train_df[\"price\"]\n",
    "display(data_train_ft)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here I did a training/test split to get different data to 1) find the best model and 2) use the other set to fit the model. I tried to test linear regression and several different regression models including KNN, Lasso, and Ridge. I ended up commenting out KNN because it took a long time and was never being chosen as the best model anyway. The model with the lowest RMSE would be chosen as the best model and would be used to fit the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#----------------------------------\n",
    "# Split into training/testing data \n",
    "#----------------------------------\n",
    "(data_train_plus_validation_ftrs, \n",
    " data_test_ftrs,\n",
    " data_train_plus_validation_tgt, \n",
    " data_test_tgt) = skms.train_test_split(data_train_ft,\n",
    "                                   data_train_tgt,\n",
    "                                   test_size=.20)\n",
    "\n",
    "# separate training/validation sets\n",
    "(train_ftrs,\n",
    " validation_ftrs,\n",
    " train_tgt,\n",
    " validation_tgt) = skms.train_test_split(data_train_plus_validation_ftrs,\n",
    "                                         data_train_plus_validation_tgt,\n",
    "                                         test_size=.25)\n",
    "\n",
    "# define dictionary of models to try\n",
    "models_to_try = {'lr': linear_model.LinearRegression()}\n",
    "# add k-NN models with various values of k to models_to_try\n",
    "\"\"\"for k in range(1,16,2):\n",
    "    models_to_try[f'{k}-NN'] = neighbors.KNeighborsRegressor(n_neighbors=k)\n",
    "\"\"\"\n",
    "#LASSO\n",
    "values = [0.25, 0.50, 0.75, 1.0, 1.25, 1.50, 1.75, 2.0, 3.0, 4.0, 5.0, 10, 20]\n",
    "for alpha_value in values:\n",
    "    models_to_try[f'Lasso (C={alpha_value})'] = linear_model.Lasso(alpha=alpha_value)\n",
    "#RIDGE\n",
    "alpha_value = 0.1\n",
    "for alpha_value in values:\n",
    "    models_to_try[f'ridge (C={alpha_value})'] = linear_model.Ridge(alpha=alpha_value)\n",
    "\n",
    "#TEST PREDICTION\n",
    "model_rmse = {}\n",
    "for model_name in models_to_try:\n",
    "    fit = models_to_try[model_name].fit(train_ftrs, train_tgt)\n",
    "    preds  = models_to_try[model_name].predict(validation_ftrs)\n",
    "    rmse = np.sqrt(metrics.mean_squared_error(validation_tgt, preds))\n",
    "    model_rmse[model_name] = rmse\n",
    "    print(f'{model_name} RMSE: {rmse:.4f}')\n",
    "\n",
    "# get model with lowest error\n",
    "best_model_name = min(model_rmse,key=model_rmse.get)\n",
    "print(f'\\nBest model: {best_model_name}; RMSE: {model_rmse[best_model_name]:.2f}')\n",
    "best_model = models_to_try[best_model_name]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Models: Here I just modified the test data to be able to make predictions with my model just like how I modified the features earlier. I then fit the data on the model, made my predictions, and recorded them on a csv file to submit. I attempted to standardize the data but it did not result in a better score so I got rid of it. Cross validation was not used. Another thing to note is that I was not able to drop any NAN values because I needed to make predictions on ALL the prices so I just replaced them with the mean."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# This is just using the test.csv to setup a dataframe of the correct size\n",
    "# and indicies (the \"id\" field).\n",
    "make_submission_df = pd.read_csv(\"cars_reg_test.csv\")\n",
    "submission_ftrs = make_submission_df[features2]\n",
    "\n",
    "#Modify testing data\n",
    "#Remove all rows with NaN values\n",
    "submission_ftrs[\"condition\"] = submission_ftrs[\"condition\"].replace(\"new\", 5).replace(\"like new\", 4).replace(\"excellent\", 3).replace(\"good\", 2).replace(\"fair\", 1).replace(\"salvage\", 0).astype(float)\n",
    "submission_ftrs[\"condition\"].fillna(submission_ftrs[\"condition\"].mean(), inplace = True)\n",
    "submission_ftrs[\"cylinders\"] = submission_ftrs[\"cylinders\"].replace(\"12 cylinders\", 12).replace(\"10 cylinders\", 10).replace(\"8 cylinders\", 8).replace(\"6 cylinders\", 6).replace(\"5 cylinders\", 5).replace(\"4 cylinders\", 4).replace(\"3 cylinders\", 3).replace(\"other\", 0).astype(float)\n",
    "submission_ftrs[\"cylinders\"].fillna(submission_ftrs[\"cylinders\"].mean(), inplace = True)\n",
    "submission_ftrs[\"title_status\"] = submission_ftrs[\"title_status\"].replace(\"clean\", 3).replace(\"rebuilt\", 2).replace(\"salvage\", 1).replace(\"missing\", 0).replace(\"parts only\", 0).replace(\"lien\", 0).astype(float)\n",
    "submission_ftrs[\"title_status\"].fillna(submission_ftrs[\"title_status\"].mean(), inplace = True)\n",
    "submission_ftrs[\"year\"].fillna(submission_ftrs[\"year\"].mean(), inplace = True)\n",
    "submission_ftrs[\"odometer\"].fillna(submission_ftrs[\"odometer\"].mean(), inplace = True)\n",
    "\n",
    "#onehot function to create one-hot dataframes\n",
    "def onehot(x,str):\n",
    "    onehot = x[['id',str]]\n",
    "    encoder = OneHotEncoder(handle_unknown='ignore')\n",
    "    encoder_df = pd.DataFrame(encoder.fit_transform(onehot[[str]]).toarray())\n",
    "    final_df = onehot.join(encoder_df)\n",
    "    final_df.drop('id', axis=1, inplace=True)\n",
    "    final_df.drop(str, axis=1, inplace=True)\n",
    "    return(final_df)\n",
    "''''''\n",
    "# working the onehot function for each column\n",
    "kind = onehot(make_submission_df, 'type')\n",
    "#Assigning values so I don't have to write it all out again\n",
    "f = [\"1\", \"2\", \"3\", \"4\", \"5\", \"6\", \"7\", \"8\", \"9\", \"10\", \"11\", \"12\", \"13\"]\n",
    "kind.columns = f\n",
    "#Adding each column back to the original df\n",
    "for col in f:\n",
    "    submission_ftrs[col] = kind[col]\n",
    "#submission_ftrs.drop(\"type\", axis=1, inplace=True)\n",
    "display(submission_ftrs)\n",
    "\n",
    "#converts to float16 types\n",
    "for feature in features:\n",
    "    if feature != 'price':\n",
    "        submission_ftrs[feature] = submission_ftrs[feature].convert_dtypes(np.float16)\n",
    "        submission_ftrs[feature] = submission_ftrs[feature].convert_dtypes(np.float16)\n",
    "\n",
    "# drop all columns except 'id'\n",
    "make_submission_df = make_submission_df[['id']]\n",
    "# make sure the column of ID's that we just read in is the index column\n",
    "make_submission_df = make_submission_df.set_index('id')\n",
    "\n",
    "fit = best_model.fit(data_test_ftrs, data_test_tgt)\n",
    "predictions = best_model.predict(submission_ftrs)\n",
    "\n",
    "# Here, you add your predictions to the dataframe\n",
    "make_submission_df['price'] = predictions\n",
    "\n",
    "# Either one of these will work\n",
    "# The first one will round all floating point numbers to 2 decimals\n",
    "# Makes it easier to look at.\n",
    "make_submission_df.to_csv('submission.csv',sep=',', float_format='%.2f')\n",
    "#make_submission_df.to_csv('submission.csv',sep=',')\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "c8610bb4a80ac6b87b3433d5c4f8bc5eaf2129f71828e941c66594fe0173a8dd"
  },
  "kernelspec": {
   "display_name": "Python 3.9.7 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
